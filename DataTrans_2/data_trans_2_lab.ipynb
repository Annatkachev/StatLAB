{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89df1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from numpy.random import uniform\n",
    "from numpy.random import normal\n",
    "from numpy.random import exponential\n",
    "from numpy.random import lognormal\n",
    "\n",
    "from numpy.random import choice\n",
    "from numpy.random import permutation\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a347b66",
   "metadata": {},
   "source": [
    "## DATA TRANS 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d0e1b4",
   "metadata": {},
   "source": [
    "    1. \n",
    "    Consider the dataset 'somescales_dementia.csv', which contains information on healthy scales and BMI for a set of nonagenerians(Mini-Mental State Examination (MMSE), The Short Physical Performance Battery (SPPB), Charlson Comorbidity Index (Charlson, multiplied-1 to retain consistent direction)). Assume you want to analysis the variance between samples using these four feautures. Using PCA as an example, perform the analysis on the original data scales and Z-score data. For the first PC component, extract feature coefficients representing the linear transformation. What is their relationship to feature variance? For this data, would you recommend using original data or Z-score data for analysing the variation between samples, such as PCA analysis or perhaps cluster analysis?\n",
    "    \n",
    "    \n",
    "    2.\n",
    "    Consider the 'proteins_SZL.csv','proteins_samples_SZL.csv' dataset, which contains protein levels in layers of two cortical brain regions. \n",
    "    \n",
    "    A. Should you log transform the data as a first step in your analysis? Perform a sample normalization, by assuming, for each sample, the same median value of features. Visualize the sample distributions before and after normalization. Perform PCA analysis, color points by \"Layer\" factor and compare results before and after normalization. \n",
    "    \n",
    "    B. Perform statistical analysis to find differences in protein levels in Region 9 in layers between layers L2 and L4, using FDR multiple testing correction with a 25% cutoff. How much significant proteins fo you obtain before and after normalizing the data? \n",
    "    \n",
    "    C. Visualize the dependence between sample median and layer factor. What dependence do you observe? Do you recomend using normalization for this data? What considerations would you keep in mind when comparing layers L2 and L4 for normalized data? What about layers L2 and L1?\n",
    "    \n",
    "    3. Consider the 'blooddf.csv', 'bloodsamples.csv' dataset, which contains metabolite levels in blood plasma. \n",
    "   \n",
    "    A. Should you log transform the data as a first step in your analysis?  Perform a sample normalization, by assuming, for each sample, the same mean value of features. Let's say you want to study the association between blood plasma levels and BMI. Is there a dependence between the mean value of features and sample BMI? Vizualize this dependence. \n",
    "    \n",
    "    B. Perfrom statistical analysis to find differences in metabolite levels between people with low BMI (<20) and high BMI (>35), using data before and after sample normalization. What are the results that you obtain? For example, would you consider the following features: 'feature1', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7','feature8', 'feature9', to be increased or decreased in people with high BMI (be that statistically significanly or not)? Would you recomend using sample normalization for the analysis of the association between blood metabolite levels and BMI?\n",
    "    \n",
    "    4. Consider the 'blooddf.csv', 'bloodsamples.csv' dataset. \n",
    "    A. Use only first 25 metabolite features to predict whether a subject is male or female, using logistic regression. Perform cross-validation to assess the performance of your model, and report sensitivity, specificity, and AUC values. \n",
    "    B. Change the decision boundary of prediction score from 0.5 to 0.75. What sensitivity and specificity estimates do you obtain in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14079dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/anna/PROJECTS2025/StatDatAnLAB/DataTrans/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be0220",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1149faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path='https://raw.githubusercontent.com/Annatkachev/StatLAB/refs/heads/main/DataTrans_2/'\n",
    "data=pd.read_csv(path+'somescales_dementia.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2499b145",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3b404ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path='https://raw.githubusercontent.com/Annatkachev/StatLAB/refs/heads/main/DataTrans_1/'\n",
    "data=pd.read_csv(path+'proteins_SZL.csv',index_col=0)\n",
    "samples=pd.read_csv(path+'proteins_samples_SZL.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e976039",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0789a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path='https://raw.githubusercontent.com/Annatkachev/StatLAB/refs/heads/main/DataTrans_1/'\n",
    "data=pd.read_csv(path+'blooddf.csv',index_col=0)\n",
    "samples=pd.read_csv(path+'bloodsamples.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948238b",
   "metadata": {},
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c536ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(path+'blooddf.csv',index_col=0)\n",
    "samples=pd.read_csv(path+'bloodsamples.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a5a251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f57a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
